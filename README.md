# 招聘爬虫
下载后直接使用pycharm新建项目，然后加入这些文件，从1-18依次按照顺序运行，各个文件详细作用如下
*********由于boss反爬更新，更新最新boss爬虫文件（12-new）原文件（12）弃用，新文件可以无限添加boss链接*******
整个项目的主要流程是从多个招聘网站爬取招聘信息，将这些信息保存到 CSV 文件中，然后合并这些 CSV 文件，对合并后的数据进行去重处理，最后对清洗后的数据进行可视化分析。配置文件则用于管理项目的开发环境和版本控制。
1. 配置浏览器环境（1.py）
目标：确保爬虫程序能够正确调用 Chrome 浏览器进行网页自动化操作。
详细步骤：
路径设置：使用 DrissionPage 库的 ChromiumOptions 类，手动指定 Chrome 浏览器的可执行文件路径（如 C:\Program Files\Google\Chrome\Application\Chrome.exe）。
配置保存：将路径配置保存为项目默认设置，后续爬虫脚本无需重复指定，直接调用已保存的配置。
依赖：需提前安装 DrissionPage 库（pip install DrissionPage）。
作用：为后续爬虫脚本提供稳定的浏览器环境，避免因路径问题导致程序启动失败。
2. 从多个招聘网站爬取数据（2-13.py）
目标：从不同招聘平台获取全面的岗位信息，涵盖多种编程语言和技术领域。
详细步骤：
2-11.py（51job 网站爬虫）
分工协作：每个脚本负责爬取特定领域的岗位（如 Java、Python、C# 等），避免单个脚本负担过重。
爬取流程：
URL 构造：基于关键词（如 java）生成搜索 URL，指定搜索范围（如全国）。
页面导航：使用浏览器自动化工具打开 URL，循环翻页（默认 50 页）。
数据提取：监听页面数据包，解析 JSON 格式的岗位信息，提取字段包括：
岗位基本信息：名称、地区、城区、经验、学历、薪水、发布时间。
公司信息：名称、类型、规模。
职位标签：技能关键词（如 "Python"、"数据分析"）。
数据存储：将提取的信息写入独立 CSV 文件（如 qc1.csv~qc10.csv）。
12.py（Boss 直聘爬虫）
多关键词覆盖：爬取包括 Python、Java、测试工程师等 12 类岗位。
技术特点：
监听 search/joblist.json 接口，直接获取结构化数据。
增加请求间隔（0.5 秒 / 页），降低被反爬机制拦截的风险。
提取特有字段：岗位联系人、联系人职位、公司福利。
输出文件：boss.csv。
13.py（猎聘网爬虫）
API 调用：通过 POST 请求直接调用猎聘网后端 API（api-c.liepin.com）。
请求伪装：设置复杂请求头（包含 Cookie、User-Agent 等），模拟真实浏览器行为。
异常处理：捕获网络请求异常（如超时、404）和数据解析异常（如字段缺失），确保爬虫稳定运行。
输出文件：lp.csv。
3. 合并所有爬取的数据（14-15.py）
目标：整合来自不同平台、不同领域的岗位数据，形成统一数据集。
详细步骤：
14.py（合并 51job 数据）
输入：qc1.csv~qc10.csv（共 10 个文件，来自 51job 不同关键词搜索）。
处理：使用 pandas.concat() 按行合并所有 DataFrame，保留原始列结构。
输出：qc.csv（包含 51job 全量数据）。
15.py（合并跨平台数据）
输入：boss.csv（Boss 直聘）、lp.csv（猎聘网）、qc.csv（51job）。
处理：再次使用 pandas.concat() 合并三个平台的数据。
输出：ccc.csv（包含所有爬取的岗位信息）。
注意：合并时保留所有原始字段，不同平台缺失的字段自动填充为 NaN。
4. 数据清洗（去重，16.py）
目标：消除重复数据，提高数据集质量。
详细步骤：
数据读取：读取合并后的 ccc.csv 文件。
去重策略：基于整行数据进行比对（默认保留最后出现的记录）。
假设重复数据为完全相同的行（包括岗位名称、公司、薪资等全部字段）。
结果保存：将去重后的数据写入 ddd.csv。
效果：减少数据分析时的冗余计算，提升后续可视化的准确性。
5. 数据分析与可视化（18.py）
目标：通过图表直观展示招聘市场的趋势和分布。
详细步骤：
数据读取：读取清洗后的 ddd.csv 文件。
分析维度：
地区分布：统计各城市岗位数量占比（如北京、上海、广州）。
经验要求：统计不同工作经验（如 1-3 年、3-5 年）的岗位需求。
学历要求：统计不同学历（如本科、硕士）的岗位占比。
可视化实现：
使用 pyecharts 库生成三种交互式图表：
饼图：展示地区分布，支持悬停查看具体比例。
柱状图：对比不同经验要求的岗位数量。
折线图：展示学历要求的分布趋势。
图表配置：设置标题、图例位置、标签格式等，提升可读性。
结果输出：生成三个 HTML 文件（如 pie_招聘地区分布情况_ddd.html），可在浏览器中直接打开查看。
价值：为求职者提供市场参考，辅助企业制定招聘策略。
整体技术链路
环境层：Chrome 浏览器 + DrissionPage 自动化框架。
数据采集层：多脚本分工爬取不同平台数据，兼顾全面性与效率。
数据处理层：pandas 进行数据合并、清洗，保证数据质量。
数据分析层：pyecharts 生成直观可视化图表，支持决策参考。
优势：模块化设计使各环节可独立维护，扩展性强（如新增爬取平台只需添加对应脚本）。
